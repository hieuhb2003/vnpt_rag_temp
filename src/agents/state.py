# =============================================================================
# Agent State - Shared state across all agents in the RAG pipeline
# =============================================================================
from typing import TypedDict, Annotated, Optional, Any, List, Dict
from langgraph.graph import add_messages
from uuid import uuid4
from datetime import datetime
from enum import Enum


# =============================================================================
# Query Complexity Enum
# =============================================================================

class QueryComplexity(str, Enum):
    """Classification of query complexity for routing."""
    SIMPLE = "simple"        # Single-fact lookup, straightforward
    MODERATE = "moderate"    # Multi-fact, comparative, or aggregation
    COMPLEX = "complex"      # Multi-step, decomposition required


# =============================================================================
# Agent State TypedDict
# =============================================================================

class AgentState(TypedDict):
    """
    Shared state across all agents in the LangGraph workflow.

    This state flows through all agents in the RAG pipeline:
    router -> planner -> retriever -> grader -> synthesizer -> verifier
    """

    # =========================================================================
    # Query Information
    # =========================================================================
    query_id: str
    """Unique identifier for this query (UUID)."""

    original_query: str
    """The original user query as submitted."""

    rewritten_query: Optional[str]
    """Query rewritten by RouterAgent for better retrieval."""

    query_type: Optional[str]
    """Type classification: factual, procedural, comparative, aggregation, etc."""

    complexity: Optional[str]
    """Query complexity: simple, moderate, or complex (QueryComplexity)."""

    # =========================================================================
    # Query Decomposition (for complex queries)
    # =========================================================================
    is_decomposed: bool
    """Whether the query was decomposed into sub-queries."""

    sub_queries: List[Dict[str, Any]]
    """List of sub-query dictionaries from PlannerAgent.
    Each contains: id, query, query_type, dependencies, etc."""

    sub_query_results: Dict[int, List[Dict[str, Any]]]
    """Results for each sub-query, keyed by sub-query index."""

    # =========================================================================
    # Execution Planning
    # =========================================================================
    execution_plan: Optional[List[Dict[str, Any]]]
    """Execution plan from PlannerAgent.
    Each step contains: step, query, collection, top_k, filters, etc."""

    # =========================================================================
    # Retrieval Results
    # =========================================================================
    retrieved_chunks: List[Dict[str, Any]]
    """List of retrieved chunk dictionaries.
    Each contains: chunk_id, content, document_id, section_id, score, metadata, etc."""

    retrieval_scores: List[float]
    """List of retrieval scores corresponding to retrieved_chunks."""

    # =========================================================================
    # Answer Synthesis
    # =========================================================================
    draft_answer: Optional[str]
    """Draft answer generated by SynthesizerAgent (before verification)."""

    final_answer: Optional[str]
    """Final answer after verification (may include warnings)."""

    citations: List[Dict[str, Any]]
    """List of citation dictionaries for sources used.
    Each contains: section_id, document_id, document_title, section_heading,
    content_snippet, relevance_score, etc."""

    # =========================================================================
    # Verification Results
    # =========================================================================
    is_grounded: Optional[bool]
    """Whether the answer is grounded in retrieved sources."""

    verification_tier: Optional[int]
    """Which verification tier was used: 1 (semantic) or 2 (LLM)."""

    unsupported_claims: List[str]
    """List of claims in the answer not supported by sources."""

    # =========================================================================
    # Grading / Quality Control
    # =========================================================================
    relevance_scores: List[float]
    """Relevance scores (0-10) assigned by GraderAgent for each chunk."""

    needs_more_retrieval: bool
    """Whether GraderAgent determined more retrieval is needed."""

    retrieval_retries: int
    """Number of retrieval retry attempts performed."""

    # =========================================================================
    # Flow Control
    # =========================================================================
    current_step: str
    """Current step in the pipeline: start, routed, planned, retrieved, graded,
    synthesized, verified, error."""

    error: Optional[str]
    """Error message if an error occurred in any agent."""

    should_escalate: bool
    """Whether this query should be escalated to human support."""

    # =========================================================================
    # Metadata
    # =========================================================================
    start_time: float
    """Timestamp when query processing started (unix timestamp)."""

    messages: Annotated[List[Any], add_messages]
    """LangGraph messages list for conversation history."""


# =============================================================================
# Initial State Creation
# =============================================================================

def create_initial_state(query: str, conversation_history: Optional[List[str]] = None) -> AgentState:
    """
    Create initial state for a new query.

    Args:
        query: The user's original query
        conversation_history: Optional list of previous conversation messages

    Returns:
        AgentState: Initial state dictionary with all fields populated
    """
    return AgentState(
        # Query info
        query_id=str(uuid4()),
        original_query=query,
        rewritten_query=None,
        query_type=None,
        complexity=None,

        # Decomposition
        is_decomposed=False,
        sub_queries=[],
        sub_query_results={},

        # Execution plan
        execution_plan=None,

        # Retrieval
        retrieved_chunks=[],
        retrieval_scores=[],

        # Synthesis
        draft_answer=None,
        final_answer=None,
        citations=[],

        # Verification
        is_grounded=None,
        verification_tier=None,
        unsupported_claims=[],

        # Grading
        relevance_scores=[],
        needs_more_retrieval=False,
        retrieval_retries=0,

        # Flow control
        current_step="start",
        error=None,
        should_escalate=False,

        # Metadata
        start_time=datetime.utcnow().timestamp(),
        messages=[]
    )


# =============================================================================
# State Update Helpers
# =============================================================================

def update_state_step(state: AgentState, step: str, error: Optional[str] = None) -> AgentState:
    """
    Update state's current step and optionally set error.

    Args:
        state: Current agent state
        step: New step name
        error: Optional error message

    Returns:
        Updated state dictionary
    """
    state["current_step"] = step
    if error:
        state["error"] = error
    return state


def merge_retrieval_results(
    state: AgentState,
    new_chunks: List[Dict[str, Any]],
    new_scores: List[float]
) -> AgentState:
    """
    Merge new retrieval results with existing ones.

    Args:
        state: Current agent state
        new_chunks: New chunks to add
        new_scores: New scores to add

    Returns:
        Updated state with merged results
    """
    state["retrieved_chunks"].extend(new_chunks)
    state["retrieval_scores"].extend(new_scores)

    # Deduplicate by chunk_id
    seen = set()
    unique_chunks = []
    unique_scores = []

    for chunk, score in zip(state["retrieved_chunks"], state["retrieval_scores"]):
        chunk_id = chunk.get("chunk_id")
        if chunk_id and chunk_id not in seen:
            seen.add(chunk_id)
            unique_chunks.append(chunk)
            unique_scores.append(score)
        elif not chunk_id:
            # Keep chunks without ID
            unique_chunks.append(chunk)
            unique_scores.append(score)

    state["retrieved_chunks"] = unique_chunks
    state["retrieval_scores"] = unique_scores

    return state


# =============================================================================
# State Serialization for Logging
# =============================================================================

def state_to_log_dict(state: AgentState) -> Dict[str, Any]:
    """
    Convert state to a dictionary suitable for logging.

    Sensitive or large fields are truncated or omitted.

    Args:
        state: Current agent state

    Returns:
        Dictionary with state information for logging
    """
    return {
        "query_id": state["query_id"],
        "query": state["original_query"][:100] + "..." if len(state["original_query"]) > 100 else state["original_query"],
        "current_step": state["current_step"],
        "complexity": state.get("complexity"),
        "is_decomposed": state["is_decomposed"],
        "chunks_retrieved": len(state["retrieved_chunks"]),
        "has_answer": state["draft_answer"] is not None,
        "is_verified": state["is_grounded"] is not None,
        "has_error": state["error"] is not None,
        "elapsed_ms": (datetime.utcnow().timestamp() - state["start_time"]) * 1000
    }
